# Defining terms

## Benchmarks and evals

In the ancestral environment, ML engineers thought a lot about _benchmarks_, which were question answer pairs that served to elicit capabilities. Back then, we would use these to _train_ (increase capabilities) provided that we cleanly separated train (for taking losses and doing gradient updates), test (for checking how the gradient updates did), and validation sets (after all is said and done, the "hold-out" set).

Lately, machine learning has pivoted a bit toward _evals_, which are like benchmarks but where the ground truth is a _process_. In evals, we allow this process to be more expensive than checking integer equality. For example, TODO. 

## Thesis: An **eval** need not ship a ground truth when the grader is very high quality

TODO establish the eval grader problem with examples. TODO tee it up so that formal verification toolcalls emerge as a kill grader. 

## Reinforcement learning
