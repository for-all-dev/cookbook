# Defining terms

In the ancestral environment, ML engineers thought a lot about _benchmarks_, which were question answer pairs that served to elicit capabilities. Back then, we would use these to _train_ (increase capabilities) provided that we cleanly separated train (for taking losses and doing gradient updates), test (for checking how the gradient updates did), and validation sets (after all is said and done, the "hold-out" set).

Lately, machine learning has pivoted a bit toward _evals_, which are like benchmarks but where the ground truth is a _process_. 

## Thesis: An **eval** need not ship a ground truth when the grader is very high quality

TODO

