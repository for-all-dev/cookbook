# Defining terms

## Benchmarks and evals

In the ancestral environment, ML engineers thought a lot about _benchmarks_, which were question answer pairs that served to elicit capabilities. Back then, we would use these to _train_ (increase capabilities) provided that we cleanly separated train (for taking losses and doing gradient updates), test (for checking how the gradient updates did), and validation sets (after all is said and done, the "hold-out" set).

Lately, machine learning has pivoted a bit toward _evals_, which are like benchmarks but where the ground truth is a _process_. In evals, we allow this process to be more expensive than checking integer equality. For example, an eval might use an LLM-as-a-judge to grade the quality of another model's outputs, or run unit tests to verify that generated code behaves correctly, send it off to mturk and wait for it to return. 

## Thesis: An **eval** need not ship a ground truth when the grader is very high quality

The challenge with eval graders is reliability: LLM-as-a-judge can be inconsistent or biased, unit tests may have incomplete coverage, and human evaluation is expensive and subjective. Traditional benchmarks sidestep this by shipping explicit ground truth answers—but this limits the complexity of tasks we can evaluate, since constructing ground truth becomes increasingly difficult for open-ended or creative problems.

However, what if the grader were so reliable that we could trust its verdicts without pre-computed answers? Formal verification tools—like Dafny and Lean—provide exactly this property. When a proof checker verifies that a program satisfies its specification, or when a theorem prover confirms a mathematical claim, these verdicts are not heuristic judgments but mathematical certainties. A formal verification toolchain acts as a perfect grader: it's deterministic, exhaustive, and provides explanatory feedback when solutions fail (through the error message). This transforms the eval paradigm—instead of shipping question-answer pairs, we can ship question-specification pairs and let the verifier grade arbitrary solutions. 

## Reinforcement learning

Reinforcement learning (RL) trains agents by rewarding desired behaviors and penalizing undesired ones through interaction with an environment. Since [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback), we've been using RL in _posttraining_, which is distinguished from the gradient descent steps (known now as _pretraining_). In RLHF, humans grade model outputs, and these grades become rewards that shape the model's behavior through policy gradient methods like PPO.

With a grader as reliable as formal verification, the path from eval to RL environment becomes remarkably straightforward. In traditional domains, constructing an RL environment requires careful reward shaping, handling edge cases, and managing noisy feedback signals. But formal verification provides binary, deterministic rewards: a solution either satisfies the specification (reward = 1) or doesn't (reward = 0), with precise error messages to guide learning. There's no need to collect human judgments, calibrate LLM-as-a-judge, or worry about reward hacking-- since the verifier simply cannot be fooled. This means any formal verification eval can be mechanically converted into an RL environment with a perfect reward signal, making formal verification uniquely positioned in the posttraining ecosystem. 

Since this reward signal is so sparse, in practice you need to prefix your process with some supervised finetuning (SFT) on problems easy enough that you can solve at time t.
