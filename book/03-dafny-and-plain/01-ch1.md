# A loop with extra steps

[As I've said](/dafny-and-inspect/ch2#thats-just-a-loop-with-extra-steps), an MVP of an agent is a loop with extra steps. 

`inspect-ai`'s `generate()` function is a convenient abstraction, but it hides a lot of the mechanical details of the agent loop. Let's pull back the curtain and see what's really going on. As we'll see, it's mostly just a `for` loop that calls the API, checks for tool use, executes tools, and feeds the results back to the model.

## The Core Iteration Loop

Here's the heart of what frameworks hide from you—a simple for loop that repeatedly calls the API until we hit success or max iterations:

```{literalinclude} ../../evals/src/evals/dafnybench/plain/agent.py
:language: python
:start-at: # Manual iteration loop
:end-at: except anthropic.APIError as e:
:end-before: # Add assistant response
```

We pass the accumulated `messages` list on each call. Anthropic's API maintains no server-side conversation state—you must send the full history every time.

## Message History Discipline

After getting a response, we must maintain strict message pairing. The Anthropic API requires that `tool_result` messages always follow an `assistant` message containing tool uses:

```{literalinclude} ../../evals/src/evals/dafnybench/plain/agent.py
:language: python
:start-at: # Add assistant response to message history
:end-at: break
:end-before: # Process tool uses
```

If the agent ends its turn without calling tools (`end_turn`), we're done. If we get any unexpected stop reason, we bail out. Only `tool_use` means we continue processing.

## Tool Execution

When the agent requests tool use, we iterate through all tool calls in the response, execute them, and collect results:

```{literalinclude} ../../evals/src/evals/dafnybench/plain/agent.py
:language: python
:start-at: # Process tool uses
:end-at: update_code_state(messages, latest_code)
:end-before: # If verification succeeded
```

Notice the careful ordering: we append tool results as a `user` message *before* updating code state. This maintains Anthropic's message pairing requirements—`tool_result` blocks must immediately follow the assistant message that requested them.

The comment about non-cumulative insertions is important: if the agent makes multiple insertion tool calls in one turn, only the last one persists. To see cumulative effects, the agent must call `verify_dafny` or wait for the next iteration.

## Success Handling

When verification succeeds, we don't immediately exit. Instead, we make one more API call to let the model see the success message and respond naturally:

```{literalinclude} ../../evals/src/evals/dafnybench/plain/agent.py
:language: python
:start-at: # If verification succeeded, make one more API call
:end-at: break
:end-before: # Get final code from state
```

This "one more call" pattern gives the agent a chance to celebrate, explain what worked, or provide closing remarks. It's a small UX detail that makes the conversation feel complete.

## Cleanup and Return

Finally, we handle the failure case, save artifacts, and return structured results:

```{literalinclude} ../../evals/src/evals/dafnybench/plain/agent.py
:language: python
:start-at: # Get final code from state if we didn't get it from success
:end-at: return AgentResult(
```

On failure, we extract the final code from message history, run one last verification to categorize the error type (for metrics), and save the full conversation history to JSON for post-mortem analysis.

---

That's it. About 130 lines to implement what `generate()` does in one function call. The verbosity is the point—you can see exactly what's happening, when messages are added, how tool results flow back, and where state updates occur. No magic, just loops and API calls.
