# Into the code

Now we'll walk through the complete implementation of the DafnyBench evaluation using Inspect AI. The source code lives in `evals/src/evals/dafnybench/inspect_ai/` and serves as our single source of truth—this markdown file just manages the rendering.

The implementation consists of six Python modules totaling ~450 lines of code:
- **`__init__.py`** (209 lines): Task definition and scorer
- **`prompt.py`** (52 lines): System prompt for Dafny verification
- **`utils.py`** (54 lines): Code extraction and error categorization utilities
- **`dataset.py`** (31 lines): HuggingFace dataset loader
- **`tools.py`** (56 lines): The `verify_dafny` tool
- **`metrics.py`** (45 lines): Custom metrics for tracking verification performance

Let's walk through each module in detail.

## Dataset Loading

The dataset module is the simplest—it loads the DafnyBench dataset from HuggingFace and converts it to Inspect's `Sample` format:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/dataset.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/dataset.py
:linenos:
```

Each `Sample` has:
- **`input`**: The prompt with hints-removed Dafny code
- **`target`**: The ground truth verified program (not used for grading, just for reference)
- **`metadata`**: Test ID, file path, and both versions of the code

The dataset contains 782 samples. We call `load_dafnybench_dataset()` in the task definition to get all samples, then optionally slice it with `[:limit]` for testing on smaller subsets.

## The verify_dafny Tool

The heart of the evaluation is the `verify_dafny` tool. The agent calls this tool with modified Dafny code, and the tool runs the Dafny compiler to check verification:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/tools.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/tools.py
:linenos:
```

The tool performs three key checks:

1. **Anti-cheating validation** (line 28): Rejects code containing `{:verify false}`, which would bypass verification entirely
2. **Dafny execution** (lines 39-43): Runs `dafny verify` with a 30-second timeout
3. **Success detection** (line 46): Checks for return code 0 and "0 errors" in stdout

If verification succeeds, the tool returns a success message. If it fails, the tool raises a `ToolError` containing the compiler's diagnostic output. The agent receives this error message and can iterate naturally—analyzing the diagnostics and trying again with improved hints.

This is the key difference between an eval and a simple benchmark: the agent gets actionable feedback and can iterate until success. No manual loop management needed—Inspect's `generate()` solver handles the tool-calling loop automatically.

## Custom Metrics

Beyond simple accuracy, we want to understand *how* the agent solves problems. The metrics module defines three custom metrics:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/metrics.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/metrics.py
:linenos:
```

Each metric is a function that takes a list of `Score` objects and computes an aggregate statistic:

- **`verification_time()`** (lines 6-18): Average time spent verifying each sample
- **`avg_attempts()`** (lines 21-31): Average number of tool calls before success or failure
- **`error_type_distribution()`** (lines 34-45): Breakdown of error categories (invariant violations, assertion failures, etc.)

These metrics are registered with the scorer using the `@scorer(metrics=[...])` decorator, and Inspect automatically tracks them across all samples.

## System Prompt

The system prompt teaches the agent about Dafny verification. It lives in its own module for clarity:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/prompt.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/prompt.py
:linenos:
```

The prompt explains five types of verification hints the agent needs to add (invariants, assertions, pre/postconditions, termination measures), emphasizes the iterative workflow with the `verify_dafny` tool, and provides a concrete example.

Note the double braces `{{}}` in the example—these escape the braces so they're not interpreted as format string placeholders.

## Utility Functions

The utilities module provides helper functions for code extraction and error analysis:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/utils.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/utils.py
:linenos:
```

The `extract_code()` function handles the agent's completion, which might include markdown formatting or explanations. It finds code blocks marked with ` ```dafny ` or just ` ``` `, extracts the code, and returns the last match (since the model might explain its reasoning before providing the final code).

The `categorize_error()` function pattern-matches Dafny error messages to identify common verification failures: invariant violations, assertion failures, postcondition issues, etc. The category is stored in score metadata and aggregated by the `error_type_distribution` metric.

## Task Definition and Scorer

The main module brings everything together, importing from the specialized modules we just covered.

### The Scorer

Now the crucial part—the scorer that runs Dafny verification and assigns grades:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: Dafny verification scorer
:linenos:
:start-at: @scorer(
:end-at: return score
:emphasize-lines: 1-6
```

The `@scorer` decorator (lines 143-150) registers our custom metrics. Inspect will automatically compute these metrics across all samples at the end of the evaluation.

The `score()` function itself:
1. **Extracts code** from the agent's completion (line 167)
2. **Writes to temporary file** (line 175)
3. **Runs Dafny compiler** (lines 178-181)
4. **Checks for success** (line 186): Return code 0 and "0 errors" in output
5. **Returns a Score** (lines 204-218) with:
   - `value`: "C" for correct, "I" for incorrect
   - `answer`: The extracted code
   - `explanation`: Success message or first 500 chars of error output
   - `metadata`: Verification time, error type, attempts, etc.

The scorer also handles timeouts (lines 220-231) and unexpected exceptions (lines 232-244), ensuring we always return a valid score even when things go wrong.

### The Task

Finally, we define the task itself:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: DafnyBench task definition
:linenos:
:start-at: @task
:end-before: def run_dafnybench_eval(
```

The task configuration is elegant:
- **Dataset**: Load from HuggingFace, optionally limit samples for testing
- **Solver chain** (lines 286-290):
  1. `system_message()`: Add our Dafny verification prompt
  2. `use_tools()`: Register the `verify_dafny` tool
  3. `generate()`: Call the model—this handles the entire tool-calling loop automatically
- **Scorer**: Our custom `dafny_verifier()` that runs the compiler
- **Sandbox**: Set to "local" since we expect Dafny to be installed

The magic is in line 289: `generate()` with tools enabled. This single solver handles the entire agentic loop:
1. Model generates response (possibly with tool calls)
2. If tool calls present, execute them
3. Add tool results to conversation
4. Call model again
5. Repeat until model provides final answer or hits limits

No manual loop management. No checking for tool calls. No counting iterations. Inspect handles all of this.

### Programmatic Entry Point

The final function provides a Python API for running the evaluation (called by our CLI):

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: Programmatic evaluation entry point
:linenos:
:start-at: def run_dafnybench_eval(
```

This function constructs the task and calls `eval()`, which executes the evaluation and handles all logging. The CLI wrapper in `evals/src/evals/__init__.py` calls this function with parsed command-line arguments.

## Complete Architecture

Let's put it all together. Here's the evaluation flow:

1. **Dataset loading**: Load 782 Dafny programs from HuggingFace
2. **For each sample**:
   - System prompt explains verification task
   - Agent receives hints-removed program
   - Agent generates code with hints added
   - Agent calls `verify_dafny` tool to check verification
   - Tool runs Dafny compiler, returns success or diagnostics
   - Agent iterates (analyzes errors, refines hints, tries again)
   - Loop continues until success or agent stops
3. **Scoring**: Run Dafny compiler one final time on agent's solution
4. **Metrics aggregation**: Compute accuracy, avg time, avg attempts, error distribution
5. **Logging**: Inspect stores all interactions for replay in `inspect view`

The entire evaluation is ~450 lines of Python across six well-organized modules. Compare this to building from scratch with the Anthropic SDK—you'd need to implement:
- Tool calling loop with state management
- Retry logic and timeout handling
- Structured logging and metric tracking
- Score aggregation and statistics
- Web UI for replay and debugging

Inspect gives us all of this for free. We focus purely on the evaluation logic: define the task, provide tools, write a scorer. The framework handles the rest.

## Next Steps

This implementation is complete and functional. Run it yourself:

```bash
cd evals
uv run agent dafnybench --framework inspect --limit 10
```

Then explore the logs:

```bash
uv run inspect view
```

In the next chapter, we'll implement the same evaluation "rawdog" style—using just the Anthropic SDK without any framework. You'll see exactly what Inspect is abstracting away, and gain a deeper understanding of what's happening under the hood.

---

## Sources

- [Sphinx literalinclude directive documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html)
- [MyST Markdown code blocks guide](https://mystmd.org/guide/code)
- [Jupyter Book markdown files documentation](https://jupyterbook.org/v1/file-types/markdown.html)
