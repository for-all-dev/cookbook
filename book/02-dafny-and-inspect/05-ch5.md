# Into the code

Now we'll walk through the complete implementation of the DafnyBench evaluation using Inspect AI. If you'd like to clone the repo, opening up Claude Code and chatting with [the repo about `evals.dafnybench.inspect_ai`](https://github.com/for-all-dev/cookbook/tree/master/evals/src/evals/dafnybench/inspect_ai) is a great substitute for reading this chapter. If you do that, then skip to the next chapter. 

## Dataset Loading

DafnyBench can be interacted with as a [dir full of `.dfy` files](https://github.com/sun-wendy/DafnyBench/tree/main/DafnyBench/dataset) or on [HuggingFace](https://huggingface.co/datasets/wendy-sun/DafnyBench). HuggingFace is the github of ML, it serves datasets and weights (trained models), and they've become the standard setters in a few topic areas. Since we're working with blackbox/API models from Anthropic, we're not downloading weights from them. 

All we do here is convert HuggingFace's data structure to Inspect's `Sample` data structure.

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/dataset.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/dataset.py
:linenos:
```

Each `Sample` has:
- **`input`**: The prompt with hints-removed Dafny code
- **`metadata`**: Test ID, file path, and both versions of the code

Notice that Inspec's `Sample` abstraction has a **`target`** feature, but we're not using it because of what I said before: [a formal verification benchmark need not ship ground truth](/introduction/ch2).

## The `verify_dafny` Tool

In the parlance, some non-LLM process that forms a sensor-actuator pair for an LLM is called a **tool**. This can be a call to a read or write to a database, an API call to query the weather at some zip code, the reasoning trace of a _different_ LLM, or in our case the `dafny` executable. 

`sanbox().exec` is Inspect's `subprocess.run` with extra steps. It's how you run shell commands from python.

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/tools.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/tools.py
:linenos:
```

`{:verify false}` is an escape hatch in Dafny, so we'd consider it cheating if it did that (line 28). We can execute `dafny` on a single file, and you want to use `with tempfile.NamedTemporaryFile` to do this so it cleans up later. Finally, we read off the exit code to see if the LLM was successful. By convention, exit code 0 is happiness, no error message, and nonzero exit code is unhappiness (though if you're the kind of person who's happy when the world gives you helpful pointers about how to fix your mistakes, you might not view this as unhappiness)[^1].

[^1]: Notice that we don't necessarily know whether the error message will go to stdout or stderr. Lots of tools aren't completely intuitive about this, where they'll put error messages in stdout and not use stderr for anything. It feels nice to make your subprocess handler agnostic and flexible, sometimes, even if for any _one_ tool it doesn't need to be, it might help with code reuse later. 

If verification succeeds, the tool returns a success message. If it fails, the tool raises a `ToolError` containing the error message. Inspect handles the **loop**, feeding the error message back into the model. In the [next part](/dafny-and-rawdog/ch0), we will do that manually.

Recall when [I wrote](/introduction/ch2):
> Implicitly, there's a rough analogy/correspondence where benchmark is to one-shot inference call as eval is to agent. If you don't know what I mean by this, just keep reading. It will become clear! 

An agent is just looping LLM API calls with custom sensors and actuators, registered as "tool calls". In the program synthesis case, the power of your agent is monotonically proportional to the expressiveness of your error messages, and overall the width and depth of your compiletime knowledge. The reason for this is because the language model at time $t + 1$ reads the error message from what it did at time $t$[^2]:

[^2]: There's an informal sense in which the error message or return code forms a "nonnumeric reward", which doesn't literally cash out in the MDP formalism but has a loose vibe kinda like reinforcement learning. 

## System Prompt

The system prompt teaches the agent about Dafny verification. It lives in its own module for clarity:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/prompt.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/prompt.py
:linenos:
```

The prompt explains five types of verification hints the agent needs to add (invariants, assertions, pre/postconditions, termination measures), emphasizes the iterative workflow with the `verify_dafny` tool, and provides a concrete example[^3].

[^3]: Note the double braces `{{}}` in the example—these escape the braces so they're not interpreted as format string placeholders.

## Utilities

The utilities module provides helper functions for code extraction and error analysis:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/utils.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/utils.py
:linenos:
```

The `extract_code()` function handles the agent's completion, which might include markdown formatting or explanations. It finds code blocks marked with ` ```dafny ` or just ` ``` `, extracts the code, and returns the last match (since the model might explain its reasoning before providing the final code)[^4].

[^4]: Lots of people like XML markup as well, like `<dafny> ... </dafny>`. The parsing question is just as trivial in each case, I find that you have to try harder in the system prompt to do the XML version.

## Task Definition and Scorer

The main module brings everything together, importing from the specialized modules we just covered.

### The Scorer

Now the crucial part—the scorer that runs Dafny verification and assigns grades:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: Dafny verification scorer
:linenos:
:start-at: @scorer
:end-at: return score
```

The `@scorer` decorator registers built-in metrics (`accuracy()` and `stderr()`). Inspect will automatically compute these across all samples.

The `score()` function itself:
1. **Extracts code** from the agent's completion
2. **Writes to temporary file**
3. **Runs Dafny compiler** with a 30-second timeout
4. **Checks for success**: Return code 0 and "0 errors" in output
5. **Returns a Score** with:
   - `value`: "C" for correct, "I" for incorrect
   - `answer`: The extracted code
   - `explanation`: Success message or error output with categorized error type

The scorer also handles timeouts and unexpected exceptions, ensuring we always return a valid score even when things go wrong.

### The Task

Finally, we define the task itself:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: DafnyBench task definition
:linenos:
:start-at: @task
:end-before: def run_dafnybench_eval(
```

The task configuration is elegant:
- **Dataset**: Load from HuggingFace, optionally limit samples for testing
- **Solver chain** (lines 286-290):
  1. `system_message()`: Add our Dafny verification prompt
  2. `use_tools()`: Register the `verify_dafny` tool
  3. `generate()`: Call the model—this handles the entire tool-calling loop automatically
- **Scorer**: Our custom `dafny_verifier()` that runs the compiler
- **Sandbox**: Set to "local" since we expect Dafny to be installed

The magic is in line 289: `generate()` with tools enabled. This single solver handles the entire agentic loop:
1. Model generates response (possibly with tool calls)
2. If tool calls present, execute them
3. Add tool results to conversation
4. Call model again
5. Repeat until model provides final answer or hits limits

No manual loop management. No checking for tool calls. No counting iterations. Inspect handles all of this.

### Programmatic Entry Point

The final function provides a Python API for running the evaluation (called by our CLI):

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: Programmatic evaluation entry point
:linenos:
:start-at: def run_dafnybench_eval(
```

This function constructs the task and calls `eval()`, which executes the evaluation and handles all logging. The CLI wrapper in `evals/src/evals/__init__.py` calls this function with parsed command-line arguments.

## Complete Architecture

Let's put it all together. Here's the evaluation flow:

1. **Dataset loading**: Load 782 Dafny programs from HuggingFace
2. **For each sample**:
   - System prompt explains verification task
   - Agent receives hints-removed program
   - Agent generates code with hints added
   - Agent calls `verify_dafny` tool to check verification
   - Tool runs Dafny compiler, returns success or diagnostics
   - Agent iterates (analyzes errors, refines hints, tries again)
   - Loop continues until success or agent stops
3. **Scoring**: Run Dafny compiler one final time on agent's solution
4. **Metrics aggregation**: Compute accuracy and standard error
5. **Logging**: Inspect stores all interactions for replay in `inspect view`

The entire evaluation is ~350 lines of Python across five focused modules. Compare this to building from scratch with the Anthropic SDK—you'd need to implement:
- Tool calling loop with state management
- Retry logic and timeout handling
- Structured logging and metric tracking
- Score aggregation and statistics
- Web UI for replay and debugging

Inspect gives us all of this for free. We focus purely on the evaluation logic: define the task, provide tools, write a scorer. The framework handles the rest.

## Next Steps

This implementation is complete and functional. Run it yourself:

```bash
cd evals
uv run agent dafnybench --framework inspect --limit 10
```

Then explore the logs:

```bash
uv run inspect view
```

In the next chapter, we'll implement the same evaluation "rawdog" style—using just the Anthropic SDK without any framework. You'll see exactly what Inspect is abstracting away, and gain a deeper understanding of what's happening under the hood.

---

## Sources

- [Sphinx literalinclude directive documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html)
- [MyST Markdown code blocks guide](https://mystmd.org/guide/code)
- [Jupyter Book markdown files documentation](https://jupyterbook.org/v1/file-types/markdown.html)
