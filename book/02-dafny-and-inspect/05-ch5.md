# Into the code

Now we'll walk through the complete implementation of the DafnyBench evaluation using Inspect AI. If you'd like to clone the repo, opening up Claude Code and chatting with [the repo about `evals.dafnybench.inspect_ai`](https://github.com/for-all-dev/cookbook/tree/master/evals/src/evals/dafnybench/inspect_ai) is a great substitute for reading this chapter. If you do that, then skip to the next chapter. 

## Dataset Loading

DafnyBench can be interacted with as a [dir full of `.dfy` files](https://github.com/sun-wendy/DafnyBench/tree/main/DafnyBench/dataset) or on [HuggingFace](https://huggingface.co/datasets/wendy-sun/DafnyBench). HuggingFace is the github of ML, it serves datasets and weights (trained models), and they've become the standard setters in a few topic areas. Since we're working with blackbox/API models from Anthropic, we're not downloading weights from them. 

All we do here is convert HuggingFace's data structure to Inspect's `Sample` data structure.

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/dataset.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/dataset.py
:linenos:
```

Each `Sample` has:
- **`input`**: The prompt with hints-removed Dafny code
- **`metadata`**: Test ID, file path, and both versions of the code

Notice that Inspec's `Sample` abstraction has a **`target`** feature, but we're not using it because of what I said before: [a formal verification benchmark need not ship ground truth](/introduction/ch2).

## The `verify_dafny` Tool

In the parlance, some non-LLM process that forms a sensor-actuator pair for an LLM is called a **tool**. This can be a call to a read or write to a database, an API call to query the weather at some zip code, the reasoning trace of a _different_ LLM, or in our case the `dafny` executable. 

`sanbox().exec` is Inspect's `subprocess.run` with extra steps. It's how you run shell commands from python.

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/tools.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/tools.py
:linenos:
```

`{:verify false}` is an escape hatch in Dafny, so we'd consider it cheating if it did that (line 28). We can execute `dafny` on a single file, and you want to use `with tempfile.NamedTemporaryFile` to do this so it cleans up later. Finally, we read off the exit code to see if the LLM was successful. By convention, exit code 0 is happiness, no error message, and nonzero exit code is unhappiness (though if you're the kind of person who's happy when the world gives you helpful pointers about how to fix your mistakes, you might not view this as unhappiness)[^1].

[^1]: Notice that we don't necessarily know whether the error message will go to stdout or stderr. Lots of tools aren't completely intuitive about this, where they'll put error messages in stdout and not use stderr for anything. It feels nice to make your subprocess handler agnostic and flexible, sometimes, even if for any _one_ tool it doesn't need to be, it might help with code reuse later. 

If verification succeeds, the tool returns a success message. If it fails, the tool raises a `ToolError` containing the error message. Inspect handles the **loop**, feeding the error message back into the model. In the [next part](/dafny-and-rawdog/ch0), we will do that manually.

Recall when [I wrote](/introduction/ch2):
> Implicitly, there's a rough analogy/correspondence where benchmark is to one-shot inference call as eval is to agent. If you don't know what I mean by this, just keep reading. It will become clear! 

An agent is just looping LLM API calls with custom sensors and actuators, registered as "tool calls". In the program synthesis case, the power of your agent is monotonically proportional to the expressiveness of your error messages, and overall the width and depth of your compiletime knowledge. The reason for this is because the language model at time $t + 1$ reads the error message from what it did at time $t$[^2]:

[^2]: There's an informal sense in which the error message or return code forms a "nonnumeric reward", which doesn't literally cash out in the MDP formalism but has a loose vibe kinda like reinforcement learning. 

## System Prompt

The system prompt teaches the agent about Dafny verification. It lives in its own module for clarity:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/prompt.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/prompt.py
:linenos:
```

The prompt explains five types of verification hints the agent needs to add (invariants, assertions, pre/postconditions, termination measures), emphasizes the iterative workflow with the `verify_dafny` tool, and provides a concrete example[^3].

[^3]: Note the double braces `{{}}` in the example—these escape the braces so they're not interpreted as format string placeholders.

## Utilities

The utilities module provides helper functions for code extraction and error analysis:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/utils.py
:language: python
:caption: evals/src/evals/dafnybench/inspect_ai/utils.py (excerpt)
:linenos:
:lines: 9-10, 26-46
```

The `extract_code()` function handles the agent's completion, which might include markdown formatting or explanations. It finds code blocks marked with ` ```dafny ` or just ` ``` `, extracts the code, and returns the last match (since the model might explain its reasoning before providing the final code)[^4].

[^4]: Lots of people like XML markup as well, like `<dafny> ... </dafny>`. The parsing question is just as trivial in each case, I find that you have to try harder in the system prompt to do the XML version.

## Task Definition and Scorer

The main module brings everything together, importing from the specialized modules we just covered.

### The Scorer

A scorer, for us, is a simple map between the terms of the subprocess result (exit code and error message) to the terms of the metrics registered to inspect, the definition of success and failure for the overall task. By convention, Inspect uses a `C` flag for success and `I` flag for failure, which I think is correct and incorrect.

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: Dafny verification scorer
:linenos:
:start-at: @scorer
:end-at: except TimeoutError
```

The `score()` function itself:
1. **Extracts code** from the agent's completion
2. **Writes to temporary file**
3. **Runs Dafny compiler** with a 30-second timeout
4. **Checks for success**: Return code 0 and "0 errors" in output
5. **Returns a Score** with:
   - `value`: "C" for correct, "I" for incorrect
   - `answer`: The extracted code
   - `explanation`: Success message or error output with categorized error type

The scorer also handles timeouts and unexpected exceptions, ensuring we always return a valid score even when things go wrong.

The thing to notice is that we're, in this simple case, **repeating the tool call logic**. You must consider it as a distinction between `dafny` as the LLM's sensor and actuator, and `dafny` as the watcher's/supervisor's (i.e., yours and my) certificate or evidence. **So its not redundant**[^5]. Let me know if this isn't clear to you, its an important point and I'm happy to find more ways to reiterate it. 

[^5]: though you're free to implement it in a DRYer way

### The Task

Finally, we define the task itself:

```{literalinclude} ../../evals/src/evals/dafnybench/inspect_ai/__init__.py
:language: python
:caption: DafnyBench task definition
:linenos:
:lines: 101-107, 133-135, 140-150
:end-before: def run_dafnybench_eval(
```

The task configuration is elegant:
- **Dataset**: Load from HuggingFace, optionally limit samples for testing
- **Solver chain** (lines 286-290):
  1. `system_message()`: Add our Dafny verification prompt
  2. `use_tools()`: Register the `verify_dafny` tool
  3. `generate()`: Call the model—this handles the entire tool-calling loop automatically
- **Scorer**: Our custom `dafny_verifier()` that runs the compiler
- **Sandbox**: Set to "local" since we expect Dafny to be installed

The magic is in line 289: `generate()` with tools enabled. This single solver handles the entire agentic loop:
1. Model generates response (possibly with tool calls)
2. If tool calls present, execute them
3. Add tool results to conversation
4. Call model again
5. Repeat until model provides final answer or hits limits

No manual loop management. No checking for tool calls. No counting iterations. Inspect handles all of this.

