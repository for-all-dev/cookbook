# Conclusion, exercises, next steps.

We didn't actually do the task right. DafnyBench proper leaves the source of each sample mostly hardcoded, and only inserts hints. Instead, we had it regenerate _all_ the code in the completion because that is the simplest text-to-text setup. This is of course acceptable when language models are flawless, trustworthy, faithful, and not scruffy looking. But in real life, language models are only one of those things. We should be paranoid that the language model agent might take the opportunity of piping the non-hint code exactly through to make the problem easier for itself.

## Exercises

As a reminder, the exercises are all stretch goals, none of them are essential for following along with the book, and they're also all good claude-code prompts if you cloned the repo so far.

1. Make a "red team" prompt that tries to cheat and get away with it (this is especially easy given that we haven't done the proper hint-filling approach yet)
2. Implement the hint-filling technique. This is especially interesting before you see how I do it later in the next part.
3. Change the prompt and the regex to `<dafny> ... </dafny>` style. 

## Next steps

Instead of retrofitting the `inspect-ai` codebase for the hint-filling critique, we will move on. We will throw out our code up till this point and do it again _with no framework_. We'll use pure python with the [Anthropic SDK](https://pypi.org/project/anthropic/), to dispell any sense of "magic" behind the agentdev frameworks[^1] you'll use in the future. 

Once more into the, etc.

[^1]: If you know anything about webdev, think of agentdev as in the "pre-[react](https://react.dev/)" era. No dominant paradigm yet, a lot of duplicated work, but most of all lots of disagreement about when someone else's abstractions makes sense vs when you should rawdog it. 
